import time
import pickle
import torch
import faiss
from sentence_transformers import SentenceTransformer, util
import numpy as np

# è¨­å®š
query = "Hello æ—©å®‰"
top_k = 5
device = "cuda" if torch.cuda.is_available() else "cpu"

# è¼‰å…¥æ¨¡å‹
model = SentenceTransformer("Alibaba-NLP/gte-Qwen2-1.5B-instruct", trust_remote_code=True, device=device)

# ============ æ–¹æ³• Aï¼šåŸç”Ÿ cosine æŸ¥è©¢ ============
with open("sign_vectors.pkl", "rb") as f:
    sentences, embeddings, animation_paths = pickle.load(f)

if isinstance(embeddings, torch.Tensor):
    embeddings = embeddings.to(device)
query_embedding = model.encode(query, convert_to_tensor=True).to(device)

start_a = time.time()
cos_scores = util.cos_sim(query_embedding, embeddings)[0]
top_a = torch.topk(cos_scores, k=top_k)
end_a = time.time()

# ============ æ–¹æ³• Bï¼šFAISS æŸ¥è©¢ ============
index = faiss.read_index("sign_index_cosine.faiss")
with open("sign_metadata.pkl", "rb") as f:
    faiss_sentences, faiss_paths = pickle.load(f)

query_embedding_faiss = model.encode(query, convert_to_tensor=True).cpu().numpy().astype("float32")
query_embedding_faiss = query_embedding_faiss.reshape(1, -1)
faiss.normalize_L2(query_embedding_faiss)

start_b = time.time()
D, I = index.search(query_embedding_faiss, top_k)
end_b = time.time()

# ============ é¡¯ç¤ºæ¯”è¼ƒçµæœ ============
print("ğŸŸ¡ åŸç”ŸæŸ¥è©¢è€—æ™‚ï¼š{:.4f} ç§’".format(end_a - start_a))
for score, idx in zip(top_a.values, top_a.indices):
    print(f"  - {score.item():.4f}ï½œ{sentences[idx]}")

print("\nğŸ”µ FAISS æŸ¥è©¢è€—æ™‚ï¼š{:.4f} ç§’".format(end_b - start_b))
for score, idx in zip(D[0], I[0]):
    print(f"  - {score:.4f}ï½œ{faiss_sentences[idx]}")
